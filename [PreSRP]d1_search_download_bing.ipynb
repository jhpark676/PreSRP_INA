{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7YBh7xiMs3Uv"},"source":["## Make your own face dataset - Notebook 1\n","\n","\n","####**Designed by Joon Son Chung**\n","\n","This script downloads images from Bing Image Search. At the time of writing, the API is free for up to 1,000 search queries per month.\n","\n","Modify the following parameters, then click `Runtime > Run all`."]},{"cell_type":"markdown","source":["### **Section A1** - Import packages and set parameters\n","- Initialize the Colab instance.\n","- Mount Google Drive and set necessary paths.\n","- Make sure `FOLDER` exists in your Google Drive. This will not be made automatically."],"metadata":{"id":"fDxUxArHGkRf"}},{"cell_type":"code","metadata":{"id":"9p8SoHOBspK8"},"source":["from google.colab import drive\n","import os, glob, sys, numpy, cv2, random, requests, shutil, pdb, json, time\n","\n","# mount Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# path of the data directory relative to the home folder of Google Drive\n","GDRIVE_HOME = '/content/drive/MyDrive'\n","FOLDER      = 'face_dataset' # This is the directory where your files will be saved\n","\n","# this is the folder to write to\n","data_dir        = os.path.join(GDRIVE_HOME,FOLDER)\n","temp_path       = './downloaded_images'\n","assert os.path.exists(data_dir)\n","\n","# max number of images per identity\n","max_results = 150"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Task A2** - Set parameters below\n","- Set the **API key** and the **queries** (`words`)."],"metadata":{"id":"4L1a7LRyPUV9"}},{"cell_type":"code","source":["# your Bing API key (Bing Search V7)\n","############################################# TODO #############################################\n","API_KEY = \"\"\n","################################################################################################\n","\n","# keywords to search (names of people)\n","############################################# TODO #############################################\n","words = []\n","################################################################################################\n","\n","print('We are going to search and download images for',len(words),'identities')"],"metadata":{"id":"ImUFHCiiPUd0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nfrq1WWWtcC0"},"source":["### **Section A3** - Search and download script\n","- This is the tool for searching and downloading from Bing."]},{"cell_type":"code","metadata":{"id":"ZSIfmvQktcSR"},"source":["def search_and_download(term,tgt_dir,API_KEY,MAX_RESULTS=250,GROUP_SIZE=50):\n","\n","  # Saved at tgt_dir/term\n","  save_dir = os.path.join(tgt_dir,term)\n","\n","  # Make directory if missing\n","  os.makedirs(save_dir, exist_ok=True)\n","\n","  # Allowed extensions in CAPITALS\n","  allow_exts = ['.JPG','.JPEG','.PNG']\n","\n","  URL = \"https://api.bing.microsoft.com/v7.0/images/search\"\n","\n","  headers = {\"Ocp-Apim-Subscription-Key\" : API_KEY}\n","  params = {\"q\": term, \"offset\": 0, \"count\": GROUP_SIZE}\n","\n","  # make the search\n","  print(\"[INFO] searching Bing API for '{}'\".format(term))\n","  search = requests.get(URL, headers=headers, params=params)\n","  search.raise_for_status()\n","\n","  # grab the results from the search, including the total number of estimated results returned by the Bing API\n","  results = search.json()\n","  estNumResults = min(results[\"totalEstimatedMatches\"], MAX_RESULTS)\n","  print(\"[INFO] {} total results for '{}'\".format(estNumResults, term))\n","\n","  # initialize the total number of images downloaded thus far\n","  total = 0\n","\n","  # loop over the estimated number of results in `GROUP_SIZE` groups\n","  for offset in range(0, estNumResults, GROUP_SIZE):\n","\n","    # update the search parameters using the current offset, then\n","    # make the request to fetch the results\n","    print(\"[INFO] making request for group {}-{} of {}...\".format(offset, offset + GROUP_SIZE, estNumResults))\n","    params[\"offset\"] = offset\n","    search = requests.get(URL, headers=headers, params=params)\n","    search.raise_for_status()\n","    results = search.json()\n","    print(\"[INFO] saving images for group {}-{} of {}...\".format(offset, offset + GROUP_SIZE, estNumResults))\n","\n","    # loop over the results\n","    for v in results[\"value\"]:\n","\n","      # try to download the image\n","      try:\n","\n","        # determine the path to the output image\n","        ext = v[\"contentUrl\"][v[\"contentUrl\"].rfind(\".\"):]\n","        save_file = os.path.sep.join([save_dir, \"B{}{}\".format(str(total).zfill(8), ext)])\n","        save_json = os.path.sep.join([save_dir, \"B{}{}\".format(str(total).zfill(8), '.json')])\n","\n","        # if extension is not in a list, continue\n","        if ext.upper() not in allow_exts:\n","          print(\"[INFO] extension not allowed: {}\".format(v[\"contentUrl\"]))\n","          continue\n","\n","        # make a request to download the image\n","        print(\"[INFO] fetching: {}\".format(v[\"contentUrl\"]))\n","        r = requests.get(v[\"contentUrl\"], timeout=30)\n","\n","        # write the image to disk\n","        with open(save_file, \"wb\") as f:\n","          f.write(r.content)\n","        with open(save_json, \"w\") as json_file:\n","          json.dump(v, json_file, indent=2)\n","\n","      # catch any errors that would not unable us to download the image\n","      except:\n","        print(\"[INFO] skipping: {}\".format(v[\"contentUrl\"]))\n","        continue\n","\n","      # try to load the image from disk\n","      image = cv2.imread(save_file)\n","\n","      # if the image is `None`, then delete the image and the json file\n","      if image is None:\n","        print(\"[INFO] deleting: {}\".format(save_file))\n","        if os.path.exists(save_file):\n","          os.remove(save_file)\n","        if os.path.exists(save_json):\n","          os.remove(save_json)\n","        continue\n","\n","      # update the counter\n","      total += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQzAZqgVtris"},"source":["### **Section A4** - Execute download script\n","- This part executes the download script, and creates zip files for each identity."]},{"cell_type":"code","metadata":{"id":"vzfask2Fs3Do"},"source":["for word in words:\n","  # get save path\n","  zip_path = data_dir+'/B_{}'.format(word)\n","\n","  # skip if zip file already exists\n","  if os.path.exists(zip_path+'.zip'):\n","    print('[INFO] skipping {} since zip file already exists'.format(word))\n","    continue;\n","\n","  # search and download\n","  search_and_download(word,temp_path,API_KEY,MAX_RESULTS=max_results)\n","\n","  # make archive of the folder for this person\n","  shutil.make_archive(zip_path, 'zip', root_dir=os.path.join(temp_path,word))"],"execution_count":null,"outputs":[]}]}