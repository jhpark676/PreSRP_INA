{"cells":[{"cell_type":"markdown","metadata":{"id":"220g612iu1lR"},"source":["## Make your own face dataset - Notebook 2\n","\n","####**Designed by Joon Son Chung**\n","\n","- This notebook provides the script to crop face regions from the downloaded dataset and save the faces normalized to the center of the image.\n","- This method only leaves the files with only one face detection.\n","Note that the script only considers files with `.jpg` extension.\n","- The face detector can be downloaded from [here](https://mm.kaist.ac.kr/share/s3fd_facedet.zip). Upload to Google Drive > `models`.\n","- Make sure the ZIP file contains images in the following structure:\n","\n","```\n","- Name1.zip\n","  - File1.jpg\n","  - File2.jpg\n","  - File3.png\n","- Name2.zip\n","  - File4.jpg\n","  - File5.png\n","  - File6.jpg\n","```\n","\n","\n"]},{"cell_type":"markdown","source":["### **Section B1** - Import packages and set parameters\n","- Initialize the Colab instance.\n","- Mount Google Drive and set necessary paths."],"metadata":{"id":"NdEw4V6JHTA_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmRyd6HNrv12"},"outputs":[],"source":["from google.colab import drive\n","from zipfile import ZipFile\n","from tqdm import tqdm\n","import os, glob, sys, shutil, time\n","import numpy as np\n","import torch\n","import cv2\n","\n","# suppress warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# mount Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# path of the data directory relative to the home folder of Google Drive\n","GDRIVE_HOME = '/content/drive/MyDrive'\n","FOLDER      = 'face_dataset'\n","\n","# The following lines are the only parts of the code that you need to change. You can simply run the rest.\n","data_dir        = os.path.join(GDRIVE_HOME,FOLDER)\n","detector_path   = os.path.join(GDRIVE_HOME,'models/s3fd_facedet.zip') # Location of the face detector\n","orig_path       = './original_images' # Location to temporarily store the original images. No need to change this.\n","temp_path       = './cropped_images' # Location to temporarily store your cropped images. No need to change this.\n","\n","assert os.path.exists(detector_path), \"[!] Enter a valid path.\"\n","assert os.path.exists(data_dir), \"[!] Enter a valid path.\""]},{"cell_type":"markdown","source":["### **Section B2** - Extract files\n","- Extract all images and face detector model."],"metadata":{"id":"t62v4kcAHy1J"}},{"cell_type":"code","source":["zip_files = glob.glob(data_dir+'/B_*.zip')\n","\n","# Extract all ZIP files for downloaded identities\n","for file in tqdm(zip_files):\n","  with ZipFile(file, 'r') as zipObj:\n","    zipObj.extractall(os.path.join(orig_path,os.path.basename(os.path.splitext(file)[0])))\n","print('Zip extraction complete')\n","\n","# Copy the detector code and model from the first assignment to the current directory\n","with ZipFile(detector_path, 'r') as zipObj:\n","  zipObj.extractall('detectors')\n","print('Zip extraction complete')\n","\n","# Find the list of JPG and PNG files using glob\n","files = glob.glob(orig_path+'/*/*.jpg') + glob.glob(orig_path+'/*/*.png')\n","print(len(files),'original images found.')"],"metadata":{"id":"NIcLYso-Hyrc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldN1W0BX_Ufb"},"source":["### **Section B3** - Load face detector\n","- Create face detector instance and load the weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RliXhrMEvORL"},"outputs":[],"source":["sys.path.append('detectors')\n","from detectors import S3FD\n","\n","# Load the face detector (you can ignore this part)\n","DET = S3FD(device='cuda')"]},{"cell_type":"markdown","metadata":{"id":"Qqg0FXYQIp9V"},"source":["### **Section B4** - Load face detector\n","- We define the data loader for reading the images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0mNyyu3P-CqL"},"outputs":[],"source":["class your_dataset(torch.utils.data.Dataset):\n","    def __init__(self, files):\n","\n","        self.data   = files\n","\n","        print('{:d} files in the dataset'.format(len(self.data)))\n","\n","    def __getitem__(self, index):\n","\n","      fname = self.data[index]\n","\n","      try:\n","        # return image if read is successful\n","        image = cv2.imread(fname)\n","        image_np = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        return image, image_np, fname\n","      except:\n","        # return empty if not successful\n","        return np.array([]), np.array([]), fname\n","\n","    def __len__(self):\n","      return len(self.data)\n","\n","dataset = your_dataset(files)\n","loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=10)"]},{"cell_type":"markdown","metadata":{"id":"v1Zm9zicVM5X"},"source":["### **Section B5** - Crop and save\n","- We now crop the faces and save them to a temporary folder.\n","- This step should take around 10 minutes for 5,000 images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbynyGiEVNvx"},"outputs":[],"source":["pbar = tqdm(loader)\n","\n","for data in pbar:\n","\n","  # skip if there is read error\n","  if len(data[0].shape) != 4:\n","    print('Skipping {} - read error'.format(data[2]))\n","    continue\n","\n","  image     = data[0][0].numpy()\n","  image_np  = data[1][0].numpy()\n","  fname     = data[2][0]\n","\n","  bboxes = DET.detect_faces(image_np, conf_th=0.9, scales=[0.5])\n","\n","  pbar.set_description(\"{:d} faces detected in {}\".format(len(bboxes),fname))\n","\n","  ## this removes all images with no face detection or two or more face detections\n","  if len(bboxes) == 1:\n","\n","    # padding value\n","    bsi = 300\n","\n","    # find center and square size\n","    sx = int((bboxes[0][0]+bboxes[0][2])/2) + bsi\n","    sy = int((bboxes[0][1]+bboxes[0][3])/2) + bsi\n","    ss = int(max((bboxes[0][3]-bboxes[0][1]),(bboxes[0][2]-bboxes[0][0]))/1.5)\n","\n","    # pad the image\n","    image = np.pad(image,((bsi,bsi),(bsi,bsi),(0,0)), 'constant', constant_values=(110,110))\n","\n","    # crop the face\n","    face = image[int(sy-ss):int(sy+ss),int(sx-ss):int(sx+ss)]\n","\n","    # check that it is square and RGB\n","    if face.shape[0] == face.shape[1] and face.shape[0] > 10 and face.shape[2] == 3:\n","\n","      face = cv2.resize(face,(256,256))\n","      outname = fname.replace(orig_path,temp_path).replace('.png','.jpg')\n","      os.makedirs(os.path.dirname(outname),exist_ok=True)\n","      cv2.imwrite(outname,face)\n","\n","    else:\n","\n","      print('[INFO] Non square image {}'.format(fname))"]},{"cell_type":"markdown","metadata":{"id":"c3PRfKSrNCX3"},"source":["### **Section B6** - Compress to ZIP file\n","- Zip all images and save to Google Drive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJi53c8ANCu1"},"outputs":[],"source":["output_files = glob.glob(temp_path+'/*/*.jpg')\n","\n","print('{:d} cropped images found. Now zipping. '.format(len(output_files)))\n","\n","shutil.make_archive(data_dir+'/v1_cropped_data', 'zip', root_dir=temp_path)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}